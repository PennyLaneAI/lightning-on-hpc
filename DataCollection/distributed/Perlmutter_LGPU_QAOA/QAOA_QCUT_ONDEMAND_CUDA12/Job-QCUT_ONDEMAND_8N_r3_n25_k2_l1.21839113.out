Head node=nid008212::[128.55.82.89]
IP Head: 128.55.82.89:6379
Starting HEAD at nid008212
Starting WORKER 1 at nid008225 connecting to 128.55.82.89:6379
Starting WORKER 2 at nid008233 connecting to 128.55.82.89:6379
2024-02-17 13:04:48,661	INFO usage_lib.py:449 -- Usage stats collection is enabled by default without user confirmation because this terminal is detected to be non-interactive. To disable this, add `--disable-usage-stats` to the command that starts the cluster, or run the following command: `ray disable-usage-stats` before starting the cluster. See https://docs.ray.io/en/master/cluster/usage-stats.html for more details.
2024-02-17 13:04:48,661	INFO scripts.py:744 -- Local node IP: 128.55.82.89
2024-02-17 13:04:51,752	SUCC scripts.py:781 -- --------------------
2024-02-17 13:04:51,752	SUCC scripts.py:782 -- Ray runtime started.
2024-02-17 13:04:51,752	SUCC scripts.py:783 -- --------------------
2024-02-17 13:04:51,752	INFO scripts.py:785 -- Next steps
2024-02-17 13:04:51,752	INFO scripts.py:788 -- To add another node to this Ray cluster, run
2024-02-17 13:04:51,752	INFO scripts.py:791 --   ray start --address='128.55.82.89:6379'
2024-02-17 13:04:51,752	INFO scripts.py:800 -- To connect to this Ray cluster:
2024-02-17 13:04:51,752	INFO scripts.py:802 -- import ray
2024-02-17 13:04:51,752	INFO scripts.py:803 -- ray.init(_node_ip_address='128.55.82.89')
2024-02-17 13:04:51,752	INFO scripts.py:834 -- To terminate the Ray runtime, run
2024-02-17 13:04:51,752	INFO scripts.py:835 --   ray stop
2024-02-17 13:04:51,752	INFO scripts.py:838 -- To view the status of the cluster, use
2024-02-17 13:04:51,752	INFO scripts.py:839 --   ray status
2024-02-17 13:04:51,752	INFO scripts.py:952 -- --block
2024-02-17 13:04:51,752	INFO scripts.py:953 -- This command will now block forever until terminated by a signal.
2024-02-17 13:04:51,752	INFO scripts.py:956 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2024-02-17 13:04:48,650	INFO scripts.py:926 -- Local node IP: 128.55.82.96
2024-02-17 13:04:51,802	SUCC scripts.py:939 -- --------------------
2024-02-17 13:04:51,802	SUCC scripts.py:940 -- Ray runtime started.
2024-02-17 13:04:51,802	SUCC scripts.py:941 -- --------------------
2024-02-17 13:04:51,802	INFO scripts.py:943 -- To terminate the Ray runtime, run
2024-02-17 13:04:51,802	INFO scripts.py:944 --   ray stop
2024-02-17 13:04:51,803	INFO scripts.py:952 -- --block
2024-02-17 13:04:51,803	INFO scripts.py:953 -- This command will now block forever until terminated by a signal.
2024-02-17 13:04:51,803	INFO scripts.py:956 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
2024-02-17 13:04:51,790	INFO scripts.py:926 -- Local node IP: 128.55.82.100
2024-02-17 13:04:51,923	SUCC scripts.py:939 -- --------------------
2024-02-17 13:04:51,924	SUCC scripts.py:940 -- Ray runtime started.
2024-02-17 13:04:51,924	SUCC scripts.py:941 -- --------------------
2024-02-17 13:04:51,924	INFO scripts.py:943 -- To terminate the Ray runtime, run
2024-02-17 13:04:51,924	INFO scripts.py:944 --   ray stop
2024-02-17 13:04:51,924	INFO scripts.py:952 -- --block
2024-02-17 13:04:51,924	INFO scripts.py:953 -- This command will now block forever until terminated by a signal.
2024-02-17 13:04:51,924	INFO scripts.py:956 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
Starting WORKER 3 at nid008389 connecting to 128.55.82.89:6379
Starting WORKER 4 at nid008393 connecting to 128.55.82.89:6379
2024-02-17 13:04:57,942	INFO scripts.py:926 -- Local node IP: 128.55.82.180
2024-02-17 13:04:58,082	SUCC scripts.py:939 -- --------------------
2024-02-17 13:04:58,082	SUCC scripts.py:940 -- Ray runtime started.
2024-02-17 13:04:58,082	SUCC scripts.py:941 -- --------------------
2024-02-17 13:04:58,082	INFO scripts.py:943 -- To terminate the Ray runtime, run
2024-02-17 13:04:58,082	INFO scripts.py:944 --   ray stop
2024-02-17 13:04:58,082	INFO scripts.py:952 -- --block
2024-02-17 13:04:58,082	INFO scripts.py:953 -- This command will now block forever until terminated by a signal.
2024-02-17 13:04:58,082	INFO scripts.py:956 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
Starting WORKER 5 at nid008405 connecting to 128.55.82.89:6379
2024-02-17 13:05:00,744	INFO scripts.py:926 -- Local node IP: 128.55.82.186
2024-02-17 13:05:00,881	SUCC scripts.py:939 -- --------------------
2024-02-17 13:05:00,882	SUCC scripts.py:940 -- Ray runtime started.
2024-02-17 13:05:00,882	SUCC scripts.py:941 -- --------------------
2024-02-17 13:05:00,882	INFO scripts.py:943 -- To terminate the Ray runtime, run
2024-02-17 13:05:00,882	INFO scripts.py:944 --   ray stop
2024-02-17 13:05:00,882	INFO scripts.py:952 -- --block
2024-02-17 13:05:00,882	INFO scripts.py:953 -- This command will now block forever until terminated by a signal.
2024-02-17 13:05:00,882	INFO scripts.py:956 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
Starting WORKER 6 at nid008417 connecting to 128.55.82.89:6379
2024-02-17 13:05:03,818	INFO scripts.py:926 -- Local node IP: 128.55.82.192
2024-02-17 13:05:03,953	SUCC scripts.py:939 -- --------------------
2024-02-17 13:05:03,953	SUCC scripts.py:940 -- Ray runtime started.
2024-02-17 13:05:03,953	SUCC scripts.py:941 -- --------------------
2024-02-17 13:05:03,953	INFO scripts.py:943 -- To terminate the Ray runtime, run
2024-02-17 13:05:03,953	INFO scripts.py:944 --   ray stop
2024-02-17 13:05:03,953	INFO scripts.py:952 -- --block
2024-02-17 13:05:03,953	INFO scripts.py:953 -- This command will now block forever until terminated by a signal.
2024-02-17 13:05:03,953	INFO scripts.py:956 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
Starting WORKER 7 at nid008436 connecting to 128.55.82.89:6379
2024-02-17 13:05:06,534	INFO scripts.py:926 -- Local node IP: 128.55.82.201
2024-02-17 13:05:06,665	SUCC scripts.py:939 -- --------------------
2024-02-17 13:05:06,666	SUCC scripts.py:940 -- Ray runtime started.
2024-02-17 13:05:06,666	SUCC scripts.py:941 -- --------------------
2024-02-17 13:05:06,666	INFO scripts.py:943 -- To terminate the Ray runtime, run
2024-02-17 13:05:06,666	INFO scripts.py:944 --   ray stop
2024-02-17 13:05:06,666	INFO scripts.py:952 -- --block
2024-02-17 13:05:06,666	INFO scripts.py:953 -- This command will now block forever until terminated by a signal.
2024-02-17 13:05:06,666	INFO scripts.py:956 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
clusters=3,cluster_nodes=25,vertex_sep=2,layers=1
2024-02-17 13:05:25,004	INFO scripts.py:926 -- Local node IP: 128.55.82.178
2024-02-17 13:05:25,154	SUCC scripts.py:939 -- --------------------
2024-02-17 13:05:25,154	SUCC scripts.py:940 -- Ray runtime started.
2024-02-17 13:05:25,154	SUCC scripts.py:941 -- --------------------
2024-02-17 13:05:25,154	INFO scripts.py:943 -- To terminate the Ray runtime, run
2024-02-17 13:05:25,154	INFO scripts.py:944 --   ray stop
2024-02-17 13:05:25,154	INFO scripts.py:952 -- --block
2024-02-17 13:05:25,154	INFO scripts.py:953 -- This command will now block forever until terminated by a signal.
2024-02-17 13:05:25,154	INFO scripts.py:956 -- Running subprocesses are monitored and a message will be printed if any of them terminate unexpectedly. Subprocesses exit with SIGTERM will be treated as graceful, thus NOT reported.
Original_circuit:=<QuantumTape: wires=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78], params=1453>, Max_wires:=79, num. cut circuits:=45711, cut time:=71.61689424095675
Nodes in the Ray cluster:=[{'NodeID': 'd2fdfb6c68e03c0b9ec4e172a6da981438f864d8f8c98e7ba95c7481', 'Alive': True, 'NodeManagerAddress': '128.55.82.201', 'NodeManagerHostname': 'nid008436', 'NodeManagerPort': 33979, 'ObjectManagerPort': 33025, 'ObjectStoreSocketName': '/tmp/ray/session_2024-02-17_13-04-48_661843_503328/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2024-02-17_13-04-48_661843_503328/sockets/raylet', 'MetricsExportPort': 44642, 'NodeName': '128.55.82.201', 'RuntimeEnvAgentPort': 45294, 'alive': True, 'Resources': {'CPU': 4.0, 'memory': 179522367898.0, 'accelerator_type:A100': 1.0, 'object_store_memory': 76938157670.0, 'GPU': 4.0, 'node:128.55.82.201': 1.0}, 'Labels': {'ray.io/node_id': 'd2fdfb6c68e03c0b9ec4e172a6da981438f864d8f8c98e7ba95c7481'}}, {'NodeID': 'a7b593e56fad6739189afd535318280d4a9906a8b32366dfced1ba02', 'Alive': True, 'NodeManagerAddress': '128.55.82.180', 'NodeManagerHostname': 'nid008393', 'NodeManagerPort': 33113, 'ObjectManagerPort': 36767, 'ObjectStoreSocketName': '/tmp/ray/session_2024-02-17_13-04-48_661843_503328/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2024-02-17_13-04-48_661843_503328/sockets/raylet', 'MetricsExportPort': 41177, 'NodeName': '128.55.82.180', 'RuntimeEnvAgentPort': 64967, 'alive': True, 'Resources': {'CPU': 4.0, 'memory': 179753819751.0, 'accelerator_type:A100': 1.0, 'object_store_memory': 77037351321.0, 'GPU': 4.0, 'node:128.55.82.180': 1.0}, 'Labels': {'ray.io/node_id': 'a7b593e56fad6739189afd535318280d4a9906a8b32366dfced1ba02'}}, {'NodeID': '11d49d23cf7315c74ae37def2ff7490c30595a9f7689f455f06b13e7', 'Alive': True, 'NodeManagerAddress': '128.55.82.178', 'NodeManagerHostname': 'nid008389', 'NodeManagerPort': 35191, 'ObjectManagerPort': 42183, 'ObjectStoreSocketName': '/tmp/ray/session_2024-02-17_13-04-48_661843_503328/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2024-02-17_13-04-48_661843_503328/sockets/raylet', 'MetricsExportPort': 62360, 'NodeName': '128.55.82.178', 'RuntimeEnvAgentPort': 57557, 'alive': True, 'Resources': {'CPU': 4.0, 'memory': 180978237440.0, 'accelerator_type:A100': 1.0, 'object_store_memory': 77562101760.0, 'node:128.55.82.178': 1.0, 'GPU': 4.0}, 'Labels': {'ray.io/node_id': '11d49d23cf7315c74ae37def2ff7490c30595a9f7689f455f06b13e7'}}, {'NodeID': '0e2f3fc23da4b7e638e07a6fc7674080e4741811afe4af8eba4a688a', 'Alive': True, 'NodeManagerAddress': '128.55.82.100', 'NodeManagerHostname': 'nid008233', 'NodeManagerPort': 36623, 'ObjectManagerPort': 34405, 'ObjectStoreSocketName': '/tmp/ray/session_2024-02-17_13-04-48_661843_503328/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2024-02-17_13-04-48_661843_503328/sockets/raylet', 'MetricsExportPort': 64353, 'NodeName': '128.55.82.100', 'RuntimeEnvAgentPort': 63142, 'alive': True, 'Resources': {'CPU': 4.0, 'memory': 179480584192.0, 'object_store_memory': 76920250368.0, 'GPU': 4.0, 'accelerator_type:A100': 1.0, 'node:128.55.82.100': 1.0}, 'Labels': {'ray.io/node_id': '0e2f3fc23da4b7e638e07a6fc7674080e4741811afe4af8eba4a688a'}}, {'NodeID': '637d026852c8eec26089c1f2690def68068242e98d400fb1c14b9e6e', 'Alive': True, 'NodeManagerAddress': '128.55.82.186', 'NodeManagerHostname': 'nid008405', 'NodeManagerPort': 41515, 'ObjectManagerPort': 46735, 'ObjectStoreSocketName': '/tmp/ray/session_2024-02-17_13-04-48_661843_503328/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2024-02-17_13-04-48_661843_503328/sockets/raylet', 'MetricsExportPort': 56783, 'NodeName': '128.55.82.186', 'RuntimeEnvAgentPort': 63535, 'alive': True, 'Resources': {'memory': 179369256551.0, 'accelerator_type:A100': 1.0, 'object_store_memory': 76872538521.0, 'GPU': 4.0, 'CPU': 4.0, 'node:128.55.82.186': 1.0}, 'Labels': {'ray.io/node_id': '637d026852c8eec26089c1f2690def68068242e98d400fb1c14b9e6e'}}, {'NodeID': '4cb0604de0819ecce0636adb48913bd825883f3069ca874e5e1d8682', 'Alive': True, 'NodeManagerAddress': '128.55.82.89', 'NodeManagerHostname': 'nid008212', 'NodeManagerPort': 43557, 'ObjectManagerPort': 38255, 'ObjectStoreSocketName': '/tmp/ray/session_2024-02-17_13-04-48_661843_503328/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2024-02-17_13-04-48_661843_503328/sockets/raylet', 'MetricsExportPort': 64337, 'NodeName': '128.55.82.89', 'RuntimeEnvAgentPort': 54648, 'alive': True, 'Resources': {'CPU': 5.0, 'node:__internal_head__': 1.0, 'memory': 168044621620.0, 'accelerator_type:A100': 1.0, 'object_store_memory': 76304837836.0, 'node:128.55.82.89': 1.0, 'GPU': 4.0}, 'Labels': {'ray.io/node_id': '4cb0604de0819ecce0636adb48913bd825883f3069ca874e5e1d8682'}}, {'NodeID': '707dea70519bbda84a5af1d31c80d493ba4c822ebad5f37217d41efa', 'Alive': True, 'NodeManagerAddress': '128.55.82.96', 'NodeManagerHostname': 'nid008225', 'NodeManagerPort': 44949, 'ObjectManagerPort': 40675, 'ObjectStoreSocketName': '/tmp/ray/session_2024-02-17_13-04-48_661843_503328/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2024-02-17_13-04-48_661843_503328/sockets/raylet', 'MetricsExportPort': 60016, 'NodeName': '128.55.82.96', 'RuntimeEnvAgentPort': 62096, 'alive': True, 'Resources': {'CPU': 4.0, 'memory': 177796732109.0, 'accelerator_type:A100': 1.0, 'object_store_memory': 76198599475.0, 'GPU': 4.0, 'node:128.55.82.96': 1.0}, 'Labels': {'ray.io/node_id': '707dea70519bbda84a5af1d31c80d493ba4c822ebad5f37217d41efa'}}, {'NodeID': '2005c554a93c68034e4ef692ecbe6b05eb244339f378878db259a054', 'Alive': True, 'NodeManagerAddress': '128.55.82.192', 'NodeManagerHostname': 'nid008417', 'NodeManagerPort': 39533, 'ObjectManagerPort': 45195, 'ObjectStoreSocketName': '/tmp/ray/session_2024-02-17_13-04-48_661843_503328/sockets/plasma_store', 'RayletSocketName': '/tmp/ray/session_2024-02-17_13-04-48_661843_503328/sockets/raylet', 'MetricsExportPort': 63520, 'NodeName': '128.55.82.192', 'RuntimeEnvAgentPort': 40821, 'alive': True, 'Resources': {'CPU': 4.0, 'memory': 179063590093.0, 'accelerator_type:A100': 1.0, 'GPU': 4.0, 'object_store_memory': 76741538611.0, 'node:128.55.82.192': 1.0}, 'Labels': {'ray.io/node_id': '2005c554a93c68034e4ef692ecbe6b05eb244339f378878db259a054'}}]
cluster resources:={'accelerator_type:A100': 8.0, 'CPU': 33.0, 'memory': 1424009209654.0, 'node:128.55.82.180': 1.0, 'object_store_memory': 614575375562.0, 'GPU': 32.0, 'node:128.55.82.178': 1.0, 'node:128.55.82.96': 1.0, 'node:128.55.82.201': 1.0, 'node:128.55.82.192': 1.0, 'node:128.55.82.100': 1.0, 'node:128.55.82.186': 1.0, 'node:__internal_head__': 1.0, 'node:128.55.82.89': 1.0}
[36m(autoscaler +1h32m23s)[0m Tip: use `ray status` to view detailed cluster status. To disable these messages, set RAY_SCHEDULER_EVENTS=0.
[33m(autoscaler +1h32m23s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h32m56s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h33m31s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h34m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h34m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h35m17s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h35m52s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h36m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h37m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h37m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h38m13s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h38m48s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h39m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h39m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h40m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h41m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h41m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h42m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h42m55s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h43m30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h44m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h44m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h45m16s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h45m51s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h46m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h47m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h47m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h48m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h48m47s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h49m22s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h49m57s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h50m32s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h51m7s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h51m42s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h52m18s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h52m53s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h53m28s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h54m3s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h54m38s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h55m14s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h55m49s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h56m24s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h56m59s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h57m34s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h58m9s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h58m44s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h59m20s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +1h59m55s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +2h30s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +2h1m5s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +2h1m40s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +2h2m15s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +2h2m50s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +2h3m26s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +2h4m1s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +2h4m36s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +2h5m11s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +2h5m46s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
[33m(autoscaler +2h6m21s)[0m Warning: The following resource request cannot be scheduled right now: {'CPU': 1.0, 'GPU': 1.0}. This is likely due to all cluster resources being claimed by actors. Consider creating fewer actors or adding more nodes to this Ray cluster.
fragment execution time:=7508.549038782949
postproc time:=0.44008934404700994
QAOA expval:=1.3431767026076193e-11 with params:=[[7.20792567e-01 1.02761748e-04]]
